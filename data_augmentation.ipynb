{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from numpy import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = [\n",
    "    (\"i\", \"y\", \"j\"),\n",
    "    (\"ú\", \"ů\", \"u\"),\n",
    "    (\"s\", \"z\"),\n",
    "    (\"m\", \"n\"),\n",
    "    (\"ý\", \"y\", \"ej\"),\n",
    "    (\"je\", \"ě\", \"e\"),\n",
    "    (\"mě\", \"mně\", \"mne\"),\n",
    "    (\"h\", \"ch\"),\n",
    "    (\"p\", \"b\"),\n",
    "\n",
    "    (\" \", \", \"),\n",
    "    (\".\", \",\", \" \"),\n",
    "    (\"?\", \".\"),\n",
    "]\n",
    "\n",
    "defined_substitution_dict = {\n",
    "    letter: tuple(set(letter_group) - set([letter])) for letter_group in substitutions for letter in letter_group \n",
    "}\n",
    "\n",
    "\n",
    "# Insertions, deletions, substitutions, transpositions\n",
    "# Small / capital letters (Který -> který, KTerý, KTERÝ)\n",
    "# Diacritics -> no diacritics\n",
    "\n",
    "diacritics = \"á, č, ď, é, ě, í, ň, ó, ř, š, ť, ú, ů, ý, ž\".split(\", \")\n",
    "without_diacritics = list(\"acdeeinorstuuyz\")\n",
    "alphabet = \"a, á, b, c, č, d, ď, e, é, ě, f, g, h, ch, i, í, j, k, l, m, n, ň, o, ó, p, q, r, ř, s, š, t, ť, u, ú, ů, v, w, x, y, ý, z, ž,  \".split(\", \")\n",
    "\n",
    "# Double characters (r -> rr)\n",
    "\n",
    "MAX_TRANSPOSITION_DISTANCE = 2\n",
    "\n",
    "\n",
    "word_errors = {\n",
    "    \"bychom\": (\"by jsme\", \"bysme\"),\n",
    "    \"byste\": (\"by jste\",),\n",
    "    \"bys\": (\"by jsi\",)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_capitalizing(word, pattern_word):\n",
    "    if pattern_word == pattern_word.lower():\n",
    "        return word.lower()\n",
    "    if pattern_word[0] == pattern_word[0].upper() and pattern_word[1:] == pattern_word[1:].lower():\n",
    "        return word[0].upper() + word[1:]\n",
    "    \n",
    "    if pattern_word == pattern_word.upper():\n",
    "        return word.upper()\n",
    "    \n",
    "\n",
    "    min_length = min(len(word), len(pattern_word))\n",
    "    for i, char in enumerate(pattern_word[:min_length]):\n",
    "        if char == char.lower():\n",
    "            word = word[:i] + word[i].lower() + word[(i+1):]\n",
    "        else:\n",
    "            word = word[:i] + word[i].upper() + word[(i+1):]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_capital_letter(word: str):\n",
    "    return word[0] == word[0].upper()\n",
    "\n",
    "def contains_diacritics(word: str):\n",
    "    for letter in diacritics:\n",
    "        if letter in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contains_special_words(word: str):\n",
    "    for word_error in word_errors.keys():\n",
    "        if word_error in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def contains_letters_for_substitutions(word: str):\n",
    "    for letter in defined_substitution_dict.keys():\n",
    "        if letter in word.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_capital_letters(word: str):\n",
    "    probs = [0.7, 0.2, 0.1]  # (Který -> který, KTerý, KTERÝ)\n",
    "\n",
    "    if len(word) < 2:\n",
    "        probs = [1.0, 0.0, 0.0]\n",
    "\n",
    "    lower_all = lambda x: x.lower()\n",
    "    change_second = lambda x: x[:2].upper() + x[2:]\n",
    "    upper_all = lambda x: x.upper()\n",
    "\n",
    "    funcs = [lower_all, change_second, upper_all]\n",
    "\n",
    "    used_func = random.choice(funcs, p=probs)\n",
    "    return used_func(word)\n",
    "\n",
    "\n",
    "def substitute_word(word: str):\n",
    "    word_errors_list = list(word_errors.items())\n",
    "    #random.shuffle(word_errors_list)\n",
    "    \n",
    "    for word_error, replacements in word_errors_list:\n",
    "        if word_error not in word.lower():\n",
    "            continue\n",
    "        \n",
    "        error_start_index = word.lower().index(word_error)\n",
    "        if error_start_index >= 0:\n",
    "            end_index = error_start_index + len(word_error)\n",
    "            replacement = random.choice(replacements)\n",
    "\n",
    "            original_substring = word[error_start_index:end_index]\n",
    "\n",
    "            word = word[:error_start_index] + copy_capitalizing(replacement, original_substring) + word[end_index:]\n",
    "            return word\n",
    "    else:\n",
    "        raise ValueError(f'No word errors could be substituted in \"{word}\"!')\n",
    "\n",
    "\n",
    "def substitute_letter(word: str, substitution_dict: dict):\n",
    "    letters_to_substitute = dict()\n",
    "    regex_special_symbols = [\"?\", \".\", \"+\", \"*\", \")\", \"(\", \"]\", \"[\"]\n",
    "    for letter in substitution_dict:\n",
    "        if letter in word.lower():\n",
    "            letter_for_re = letter\n",
    "            if letter in regex_special_symbols:\n",
    "                letter_for_re = rf\"\\{letter}\"\n",
    "\n",
    "            letters_to_substitute[letter] = [m.start() for m in re.finditer(letter_for_re, word.lower())]\n",
    "\n",
    "    letter_to_sub = random.choice(list(letters_to_substitute.keys()))\n",
    "    letter_index = random.choice(letters_to_substitute[letter_to_sub])\n",
    "    substitute_by = random.choice(substitution_dict[letter_to_sub])\n",
    "\n",
    "    return copy_capitalizing(word[:letter_index] + substitute_by + word[letter_index+len(letter_to_sub):], word)\n",
    "\n",
    "\n",
    "def substitute_defined_letters(word: str):\n",
    "    return substitute_letter(word, defined_substitution_dict)\n",
    "\n",
    "def substitute_diacritics(word: str):\n",
    "    substitution_dict = {diac: [without_diac] for diac, without_diac in zip(diacritics, without_diacritics)}\n",
    "    substitution_dict |= {value[0]: [key] for key, value in substitution_dict.items()}\n",
    "    return substitute_letter(word, substitution_dict)\n",
    "\n",
    "\n",
    "def substitute(word: str):\n",
    "    if word == \"\":\n",
    "        return word\n",
    "    probs = [0.8, 0.3, 0.2, 0.1]  # word_sub_prob, letter_sub_prob, diacritics_sub_prob, random_substitution\n",
    "\n",
    "    if not contains_special_words(word):\n",
    "        probs[0] = 0.0\n",
    "    if not contains_letters_for_substitutions(word):\n",
    "        probs[1] = 0.0\n",
    "    if not contains_diacritics(word):\n",
    "        probs[2] = 0.0\n",
    "    \n",
    "    probs = [prob / sum(probs) for prob in probs]\n",
    "\n",
    "\n",
    "    # TODO: use probabilities based on langugage histogram for random.choice(alphabet)\n",
    "    random_substitution = lambda x: substitute_letter(\n",
    "        x, \n",
    "        {\n",
    "            random.choice(list(set(x.lower()))): [random.choice(alphabet)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    funcs = [substitute_word, substitute_defined_letters, substitute_diacritics, random_substitution]\n",
    "\n",
    "    used_func = random.choice(funcs, p=probs)\n",
    "    return used_func(word)\n",
    "\n",
    "def delete_letter(word: str):\n",
    "    if word == \"\":\n",
    "        return word\n",
    "    index_to_delete = random.choice(list(range(len(word))))\n",
    "    return word[:index_to_delete] + word[index_to_delete+1:]\n",
    "\n",
    "\n",
    "def transpose(word: str):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    \n",
    "    word_indices = list(range(len(word)))\n",
    "    first_letter_index = random.choice(word_indices)\n",
    "\n",
    "    # Find second index based on maximal transposition distance\n",
    "    new_range = list(range(max(0, first_letter_index - MAX_TRANSPOSITION_DISTANCE), min(len(word), first_letter_index + MAX_TRANSPOSITION_DISTANCE)))\n",
    "    del new_range[new_range.index(first_letter_index)]\n",
    "    second_letter_index = random.choice(new_range)\n",
    "\n",
    "    word_list = list(word)\n",
    "    word_list[first_letter_index], word_list[second_letter_index] = word_list[second_letter_index], word_list[first_letter_index]\n",
    "    return copy_capitalizing(\"\".join(word_list), word)\n",
    "\n",
    "\n",
    "\n",
    "def insert(word: str):\n",
    "    probs = [0.8, 0.2] # Duplicate letter, random letter\n",
    "\n",
    "    inserted_letter_random = random.choice(alphabet)\n",
    "    if word == \"\":\n",
    "        return inserted_letter_random\n",
    "\n",
    "    inserted_index = random.choice(list(range(len(word))))\n",
    "    inserted_letter_duplicate = word[inserted_index]\n",
    "\n",
    "    inserted_letter = random.choice([inserted_letter_duplicate, inserted_letter_random], p=probs)\n",
    "\n",
    "    changed_word = word[:inserted_index] + inserted_letter + word[inserted_index:]\n",
    "    return copy_capitalizing(changed_word, word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertion, deletion, substitution, transposition, capital_letters, no change\n",
    "EDIT_PROBABILITIES = (0.1, 0.2, 0.3, 0.2, 0.1, 0.1)\n",
    "assert sum(EDIT_PROBABILITIES) == 1.0\n",
    "\n",
    "MAX_CHANGES_PER_STRING = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_changes(word: str):\n",
    "    changes = [insert, delete_letter, substitute, transpose, change_capital_letters, lambda x: x]\n",
    "    for _ in range(min(len(word), MAX_CHANGES_PER_STRING)):\n",
    "        changing_func = random.choice(changes, p=EDIT_PROBABILITIES)\n",
    "        try:\n",
    "            word = changing_func(word)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    word = \"Ó, náhlý déšť již zvířil prach a čilá laň teď běží s houfcem gazel Ualdewara k exkluzívním úkrytům! Ty bychom neměli zmeškat\"\n",
    "\n",
    "    apply_changes(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['target'],\n",
       "        num_rows: 1641471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset = load_dataset(\"data/czech_news_dataset_v2\").remove_columns(\n",
    "    [\"headline\", \"brief\", \"url\", \"authors\", \"category\", \"comments_num\", \"server\", \"category_unclean\", 'day_of_week', \"date\", \"authors_cum_gender\", \"authors_gender\", \"keywords\"]\n",
    ")\n",
    "news_dataset = news_dataset.rename_column(\"content\", \"target\")\n",
    "news_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end_symbols = [\"?\", \"!\", \".\"]\n",
    "sentence_end = \"</s>\"\n",
    "\n",
    "def split_to_sentences(batch):\n",
    "    sequences = batch[\"target\"]\n",
    "    sentences = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        for symbol in sentence_end_symbols:\n",
    "            sequence = sequence.replace(symbol, f\"{symbol}{sentence_end}\")\n",
    "            sequence = sequence.replace(f\"{sentence_end} \", sentence_end)\n",
    "    \n",
    "        sentences += sequence.split(sentence_end)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence.strip()) > 1]\n",
    "    batch[\"target\"] = sentences\n",
    "    return batch\n",
    "\n",
    "def add_errors(sequence_dict):\n",
    "    error_sentence = apply_changes(sequence_dict[\"target\"])\n",
    "    sequence_dict[\"source\"] = error_sentence\n",
    "    return sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:07<00:00, 12765.74 examples/s]\n",
      "Map:  60%|█████▉    | 1267055/2118584 [25:23<15:46, 899.36 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad escape (end of pattern) at position 0\n",
      "bad escape (end of pattern) at position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  60%|█████▉    | 1267353/2118584 [25:24<16:28, 861.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad escape (end of pattern) at position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  62%|██████▏   | 1315713/2118584 [26:17<14:55, 896.54 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad escape (end of pattern) at position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  82%|████████▏ | 1733282/2118584 [34:23<08:11, 783.83 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad escape (end of pattern) at position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2118584/2118584 [41:54<00:00, 842.51 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = news_dataset[\"train\"].select(list(range(0, 100_000)))\n",
    "\n",
    "segmented_tiny = tiny_dataset.map(split_to_sentences, batched=True)\n",
    "tiny_with_errors = segmented_tiny.map(add_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = tiny_with_errors.train_test_split(test_size=0.3)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'dev': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1483008/1483008 [00:32<00:00, 45744.25 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 317788/317788 [00:02<00:00, 141371.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 317788/317788 [00:02<00:00, 157964.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_valid_dataset.save_to_disk(\"data/czech_news_errors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
